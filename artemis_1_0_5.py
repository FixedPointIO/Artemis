# -*- coding: utf-8 -*-
"""Artemis_1_0_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gG0Q0DDfB6wusVksgWHJXdPLoKzQ3N7H

# Install libraries
"""

#!pip install numpy
#!pip install scipy
!pip install nevergrad

import numpy
import scipy
#import nevergrad

"""# Set up, check folders, upload data (if not uploaded already)"""

# Import necessary libraries
import pandas as pd                 # Import the pandas library for data manipulation
from google.colab import files     # Import the files module from google.colab for file handling
import os.path                     # Import the os.path module for operating system path manipulation
import io                          # Import the io module for input/output operations

# Check if the file "data.xlsx" exists
if not os.path.exists('data.xlsx'): # Check if the file 'data.xlsx' doesn't exist in the current directory
    uploaded = files.upload()       # Prompt the user to upload files

    # Iterate through uploaded files
    for fn in uploaded.keys():
        print('User uploaded file "{name}" with length {length} bytes'.format(
            name=fn, length=len(uploaded[fn])))

        # If the uploaded file is an Excel file
        if fn.endswith('.xlsx'):
            DATA = pd.read_excel(io.BytesIO(uploaded[fn])) # Read Excel data into a DataFrame
        else:
            print(f"Unsupported file type: {fn}") # Print a message if the uploaded file is not supported
            continue
else:
    DATA = pd.read_excel('data.xlsx', sheet_name='data') # Read existing Excel data into a DataFrame

# Create images folder if it doesn't exist
if not os.path.exists('images'):     # Check if the 'images' folder doesn't exist
    os.makedirs('images')            # Create the 'images' folder

# Create numerical folder if it doesn't exist
if not os.path.exists('numerical'):  # Check if the 'numerical' folder doesn't exist
    os.makedirs('numerical')         # Create the 'numerical' folder

# Now, DATA is a DataFrame containing the contents of the uploaded or existing file

"""# Data cleaning"""

# Check if the 'growth_rate' column exists in DATA
if 'growth_rate' in DATA.columns:
    # Clean 'DATA' DataFrame by removing leading and trailing whitespaces from string values
    DATA = DATA.applymap(lambda x: x.strip() if isinstance(x, str) else x)

    # Fill missing values in 'growth_rate' column with 'Not listed'
    DATA['growth_rate'] = DATA['growth_rate'].fillna('Not listed')

    # Map values in 'growth_rate' column to either 'Fast', 'Medium', 'Slow', or 'Not listed'
    DATA['growth_rate'] = DATA['growth_rate'].apply(lambda x: x if x in ["Fast", "Medium", "Slow"] else "Not listed")

    # Replace 'NaN' values with 'Not listed' in 'growth_rate' column
    DATA['growth_rate'] = DATA['growth_rate'].replace('NaN', 'Not listed')

    # Print the modified 'DATA' DataFrame
    print(DATA)
else:
    print("No 'growth_rate' column found in the DataFrame.")

import pandas as pd

# Define a function to clean and check data in a DataFrame
def clean_and_check_data(df):
    """Clean and check the provided dataframe."""
    # Trim leading and trailing whitespaces from all cells
    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)

    # Convert all string values to lowercase
    df = df.apply(lambda x: x.str.lower() if x.dtype == "object" else x)

    # Check for duplicates in the 'treeid' column if it exists
    if 'treeid' in df.columns:
        treeid_duplicates = df[df['treeid'].duplicated(keep=False)]
    else:
        treeid_duplicates = pd.DataFrame()  # Create an empty DataFrame for duplicates

    return df, treeid_duplicates

# Assuming the DATA dataframe is available via:
# DATA = pd.read_excel('data.xlsx', sheet_name='data')

# Apply the clean_and_check_data function to the DATA DataFrame
cleaned_DATA, treeid_duplicates = clean_and_check_data(DATA)

# Check if 'treeid' exists and print the number of duplicates found
if 'treeid' in DATA.columns:
    num_treeid_duplicates = len(treeid_duplicates)
    print(f"Number of duplicate 'treeid' entries: {num_treeid_duplicates}")
else:
    print("'treeid' column not found in the DataFrame.")

"""# Visual inspection

## Plot DBH
"""

# Import necessary libraries
# Import the natural logarithm function from numpy
from numpy import log
# Import the matplotlib.pyplot module for creating plots
import matplotlib.pyplot as plt
# Import the seaborn library for enhanced data visualization
import seaborn as sns
# Import the math library for mathematical operations
import math
# Import the warnings module to suppress warnings
import warnings
warnings.filterwarnings('ignore')  # Ignore warning messages

# Add a new column for the logarithm of 'dbh' (Diameter at Breast Height)
# Apply the natural logarithm function to the 'dbh' column and store the result in a new column 'log_dbh'
DATA['log_dbh'] = log(DATA['dbh'])

# Create a boxplot of the logarithm of DBH (cm) for each tree species,
# color-coded by growth rate if the 'growth_rate' column exists
plt.figure(figsize=(10, 25))  # Set the figure size
if 'growth_rate' in DATA.columns:  # Check if 'growth_rate' column exists in DATA
    # Create a boxplot using seaborn, grouping by 'botanical_names' and color-coded by 'growth_rate'
    sns.boxplot(y="botanical_names", x="log_dbh", hue="growth_rate", data=DATA, palette=sns.color_palette("Paired"), orient="h")
else:
    # Create a basic boxplot without color coding if 'growth_rate' column doesn't exist
    sns.boxplot(y="botanical_names", x="log_dbh", data=DATA, orient="h")

# Set plot title, labels, and other formatting
plt.title('Boxplot of the natural logarithm of DBH (cm) for each tree species')
plt.ylabel('Tree Species')
plt.xlabel('Natural Logarithm of DBH (cm)')

# Save the plot as a PNG file in the 'images' folder
plt.savefig("images/log_dbh_boxplot.png")

# Display the plot
plt.show()

"""## Plot Age"""

# Import necessary libraries
import seaborn as sns            # Import seaborn for data visualization
import matplotlib.pyplot as plt  # Import matplotlib for creating plots

# Create a boxplot of age for each tree species, color-coded by growth rate (if available)
plt.figure(figsize=(10, 25))  # Set the figure size
if 'growth_rate' in DATA.columns:  # Check if 'growth_rate' column exists in DATA
    # Create a boxplot using seaborn, grouping by 'botanical_names' and color-coded by 'growth_rate'
    sns.boxplot(y="botanical_names", x="age", hue="growth_rate", data=DATA, palette=sns.color_palette("Paired"), orient="h")
else:
    # Create a basic boxplot without color coding if 'growth_rate' column doesn't exist
    sns.boxplot(y="botanical_names", x="age", data=DATA, orient="h")

# Set plot title, labels, and other formatting
plt.title('Boxplot of age for each tree species')
plt.ylabel('Tree Species')
plt.xlabel('Age')

# Save the plot as a PNG file in the 'images' folder
plt.savefig("images/age_boxplot.png")

# Display the plot
plt.show()

"""## Plot Height"""

# Import necessary libraries
import numpy as np                # Import numpy for numerical operations
import seaborn as sns             # Import seaborn for data visualization
import matplotlib.pyplot as plt   # Import matplotlib for creating plots

# Transform the height to log height using numpy's natural logarithm function
DATA['log_height'] = np.log(DATA['height'])

# Create a boxplot of log height for each tree species, color-coded by growth rate (if available)
plt.figure(figsize=(10, 25))  # Set the figure size
if 'growth_rate' in DATA.columns:  # Check if 'growth_rate' column exists in DATA
    # Create a boxplot using seaborn, grouping by 'botanical_names' and color-coded by 'growth_rate'
    sns.boxplot(y="botanical_names", x="log_height", hue="growth_rate", data=DATA, palette=sns.color_palette("Paired"), orient="h")
else:
    # Create a basic boxplot without color coding if 'growth_rate' column doesn't exist
    sns.boxplot(y="botanical_names", x="log_height", data=DATA, orient="h")

# Set plot title, labels, and other formatting
plt.title('Boxplot of log-transformed height for each tree species')
plt.ylabel('Tree Species')
plt.xlabel('Log-Transformed Height')

# Save the plot as a PNG file in the 'images' folder
plt.savefig("images/log_height_boxplot.png")

# Display the plot
plt.show()

"""## QQ Plots"""

# Import necessary libraries
import numpy as np                 # Import numpy for numerical operations
import scipy.stats as stats        # Import scipy.stats for statistical functions
import matplotlib.pyplot as plt    # Import matplotlib for creating plots

# Function to plot QQ plot for a given column
def plot_qq(data, column_name, ax):
    # Sort the data and generate theoretical quantiles for a normal distribution
    sorted_data = np.sort(data[column_name].dropna())
    theoretical_quantiles = np.sort(stats.norm.rvs(loc=sorted_data.mean(), scale=sorted_data.std(), size=len(sorted_data)))

    # Create the QQ plot
    ax.scatter(theoretical_quantiles, sorted_data)
    ax.plot([np.min((theoretical_quantiles.min(), sorted_data.min())), np.max((theoretical_quantiles.max(), sorted_data.max()))],
            [np.min((theoretical_quantiles.min(), sorted_data.min())), np.max((theoretical_quantiles.max(), sorted_data.max()))], 'r--')
    ax.set_title(f'QQ Plot for {column_name}')
    ax.set_xlabel('Theoretical Quantiles')
    ax.set_ylabel(f'Sample Quantiles of {column_name}')

# Function to perform the Anderson-Darling test and display results
def anderson_darling_test(data, column_name):
    result = stats.anderson(data[column_name].dropna())
    print(f"Results for Anderson-Darling Test on {column_name}:")
    print(f"Statistic: {result.statistic:.5f}")
    for i in range(len(result.critical_values)):
        sl, cv = result.significance_level[i], result.critical_values[i]
        if result.statistic < result.critical_values[i]:
            print(f"At significance level {sl}%, critical value: {cv:.5f} => Data looks Gaussian (fail to reject H0)")
        else:
            print(f"At significance level {sl}%, critical value: {cv:.5f} => Data does not look Gaussian (reject H0)")
    print("\n")

# Setting up the figure and axes
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))

# Generating QQ plots for 'age', 'height', and 'dbh'
plot_qq(DATA, 'age', axs[0])
plot_qq(DATA, 'height', axs[1])
plot_qq(DATA, 'dbh', axs[2])

# Performing the Anderson-Darling test for 'age', 'height', and 'dbh'
anderson_darling_test(DATA, 'age')
anderson_darling_test(DATA, 'height')
anderson_darling_test(DATA, 'dbh')

# Save the plot as a PNG file in the 'images' folder
plt.savefig("images/QQ_plots.png")

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

"""# Winsorize at 99.9%, Histogram"""

# Import necessary libraries
from scipy.stats import mstats      # Import mstats from scipy.stats for winsorizing
import matplotlib.pyplot as plt    # Import matplotlib for creating plots

# Assuming DATA dataframe is available via:
# DATA = pd.read_excel('data.xlsx', sheet_name='data')

# Winsorize at 99.9% for 'height' and 'dbh' columns
DATA['height'] = mstats.winsorize(DATA['height'], limits=[0, 0.001])
DATA['dbh'] = mstats.winsorize(DATA['dbh'], limits=[0, 0.001])

# Uncomment the next line of code to Winsorize 'age' column
# DATA['age'] = mstats.winsorize(DATA['age'], limits=[0, 0.001])

# Function to plot histograms for a given column
def plot_histogram(data, column_name, ax):
    # Plot a histogram of the data with specified settings
    data[column_name].hist(ax=ax, bins=30, edgecolor='black', alpha=0.7)
    ax.set_title(f'Histogram for {column_name}')
    ax.set_xlabel(column_name)
    ax.set_ylabel('Frequency')

# Setting up the figure and axes for histogram plots
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))

# Generating histogram plots for 'age', 'height', and 'dbh'
plot_histogram(DATA, 'age', axs[0])
plot_histogram(DATA, 'height', axs[1])
plot_histogram(DATA, 'dbh', axs[2])

# Save the plot as a PNG file in the 'images' folder
plt.savefig("images/winsorized_histograms.png")

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

"""# Chapman-Richards function fitting via Nevergrad

## Chapman-Richards growth function
$ y(t) =y_\text{max} [1 – e^{-kt}]^p $

Growth functions in general describe the change in size of an individual or population with time (Burkhart and Tomé, 2012). Assume that $y(t)$ is a tree growth variable, in our case tree DBH, and  $y_\text{max}$ is the maximum value this growth variable can take (in absolute terms for a given species in general or for a given species on a given site) then the term $[1 – e^{-kt}]^p $ is a modifier reducing the maximum growth variable to its current state at time $t$.  $k$ is an empirical growth parameter scaling the absolute growth rate. The empirical parameter $p$ is related to catabolism (destructive metabolism), which is said to be proportional to an organism’s mass. It is often restricted to a value of three or four for theoretical, biological reasons.

This code first winsorizes (at 1%) outliers for 'height', then fits the Chapman-Richards growth function to the winsorized data and plots the results in a multi-subplot figure, saving the entire figure as a PNG file in the specified directory.

Burkhart, H. and Tomé, M., 2012. Modeling forest trees and stands. Springer
"""

# Import necessary libraries
import numpy as np
import nevergrad as ng
import os
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import kstest
import requests

# Set seed for reproducibility
seed_value = 12321

# Maximum value for 'dbh' column
max_dbh = max(DATA['dbh'])

# Universal parameter ranges for A, k, and p
universal_ranges = {
        'A': (0, max_dbh),
        'k': (0.001, 3),
        'p': (0.1, 4)
}

# Function to load species-specific parameter ranges
def load_species_ranges(species_ranges_df):
    species_ranges = {}
    for _, row in species_ranges_df.iterrows():
        species_name = row['species'].strip()
        species_ranges[species_name] = {
            'A': (row['A_lower'], row['A_upper']),
            'k': (row['k_lower'], row['k_upper']),
            'p': (row['p_lower'], row['p_upper'])
        }
    return species_ranges

# Define the Huber loss function
def huber_loss(residuals, delta=1.0):
    """
    Compute the Huber loss.
    """
    abs_residuals = np.abs(residuals)
    loss = np.where(abs_residuals <= delta,
                    0.5 * abs_residuals**2,
                    delta * (abs_residuals - 0.5 * delta))
    return np.sum(loss)

# Function to calculate Huber loss for given parameters
def huber_loss_for_params(params, species_data, age_col, dbh_col):
    A, k, p = params
    predicted = chapman_richards(species_data[age_col], A, k, p)
    residuals = predicted - species_data[dbh_col]
    return huber_loss(residuals)

# Function to calculate KS test statistic for predicted and actual values
def ks_test_score(predicted_values, actual_values):
    residuals = predicted_values - actual_values
    standardized_residuals = (residuals - np.mean(residuals)) / np.std(residuals)
    ks_statistic, _ = kstest(standardized_residuals, 'norm')
    return ks_statistic

# Function to clean forestry data
def clean_forestry_data(df):
    # Strip whitespace and convert string columns to lowercase
    df = df.applymap(lambda x: x.strip() if type(x) == str else x)
    df.columns = df.columns.str.strip()

    # Convert 'height', 'age', and 'dbh' columns to numeric, handling errors
    df[['height', 'age', 'dbh']] = df[['height', 'age', 'dbh']].apply(pd.to_numeric, errors='coerce')

    # Convert string columns to lowercase
    df = df.apply(lambda x: x.str.lower() if x.dtype == "object" else x)

    # Replace double quotes with single quotes in string columns
    df = df.applymap(lambda x: x.replace('"', "'") if isinstance(x, str) else x)

    # Filter out rows with age less than or equal to 1
    df = df[df['age'] > 1]

    return df


def read_species_ranges_from_github(url):
    try:
        df = pd.read_csv(url)
        return df
    except:
        return None

def chapman_richards(age, A, k, p):
    return A * (1 - np.exp(-k * age))**p

# Define the NRMSE function
def nrmse(predicted_values, actual_values):
    rmse_val = np.sqrt(np.mean((predicted_values - actual_values)**2))
    return rmse_val / (actual_values.max() - actual_values.min())

def objective(params, species_data, age_col, dbh_col):
    A, k, p = params
    predicted = chapman_richards(species_data[age_col], A, k, p)
    residuals = predicted - species_data[dbh_col]
    return huber_loss(residuals)  # Use Huber loss instead of mean squared error

def objective_with_metrics(params, species_data, age_col, dbh_col):
    A, k, p = params
    predicted = chapman_richards(species_data[age_col], A, k, p)

    residuals = predicted - species_data[dbh_col] # Calculate residuals
    nrmse_val = nrmse(predicted, species_data[dbh_col]) # NRMSE

    # R-squared
    ss_res = np.sum(residuals**2)
    ss_tot = np.sum((species_data[dbh_col] - np.mean(species_data[dbh_col]))**2)
    r_squared = 1 - (ss_res / ss_tot)

    # Adjusted R-squared
    n = len(species_data)
    p = 3  # for A, k, p
    adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)

    # KS test score
    ks_score = ks_test_score(predicted, species_data[dbh_col])

    return nrmse_val, adj_r_squared, ks_score  # Return NRMSE, Adjusted R-squared, and KS Score

def nrmse_objective(params, species_data, age_col, dbh_col):
    A, k, p = params
    predicted = chapman_richards(species_data[age_col], A, k, p)
    return nrmse(predicted, species_data[dbh_col])  # Use NRMSE for optimization

    residuals = predicted - species_data[dbh_col] # Calculate residuals
    nrmse_val = nrmse(predicted, species_data[dbh_col]) # NRMSE

    # R-squared
    ss_res = np.sum(residuals**2)
    ss_tot = np.sum((species_data[dbh_col] - np.mean(species_data[dbh_col]))**2)
    r_squared = 1 - (ss_res / ss_tot)    # Adjusted R-squared
    n = len(species_data)
    p = 3  # for A, k, p
    adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)

    # KS test score
    ks_score = ks_test_score(predicted, species_data[dbh_col])

    return nrmse_val, adj_r_squared, ks_score  # Return NRMSE instead of RMSE

def estimate_chapman_richards(data, species_col='botanical_names', age_col='age', dbh_col='dbh', output_dir=os.getcwd(), species_ranges_df=None):

    if species_ranges_df is not None:
        species_specific_ranges = load_species_ranges(species_ranges_df)
    else:
        species_specific_ranges = {}

    # Fallback to the universal range if a species-specific range isn't provided:
    def get_parameter_range(species, parameter):
        return species_specific_ranges.get(species, {}).get(parameter, universal_ranges[parameter])

    param_results = pd.DataFrame()
    fitted_values = pd.DataFrame()

    print("Starting optimization for all species.")
    total_species = len(data[species_col].unique())
    for idx, species in enumerate(data[species_col].unique(), start=1):
        print(f"\nProcessing species: {species} ({idx} out of {total_species})")
        species_data = data[data[species_col] == species]

        if len(species_data) < 30:
            print(f"Skipping species {species} due to insufficient data.")
            continue

        print(f"Optimizing for species: {species}")

        ranges = {
            'A': get_parameter_range(species, 'A'),
            'k': get_parameter_range(species, 'k'),
            'p': get_parameter_range(species, 'p')
        }

        # Fallback to universal_ranges if species-specific ranges are not available
        if not ranges['A']:
            ranges['A'] = universal_ranges['A']
        if not ranges['k']:
            ranges['k'] = universal_ranges['k']
        if not ranges['p']:
            ranges['p'] = universal_ranges['p']

        if not ranges:
            print(f"Ranges for {species} not found in the species_ranges CSV. Skipping this species.")
            continue

        parametrization = ng.p.Instrumentation(
            A=ng.p.Scalar(lower=ranges['A'][0], upper=ranges['A'][1]),
            k=ng.p.Scalar(lower=ranges['k'][0], upper=ranges['k'][1]),
            p=ng.p.Scalar(lower=ranges['p'][0], upper=ranges['p'][1])
        )
        optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=1000)

        path_A = []
        path_k = []
        optimizer_losses = []

        def wrapped_objective(**kwargs):
            A = kwargs['A']
            k = kwargs['k']
            p = kwargs['p']
            loss = objective((A, k, p), species_data, age_col, dbh_col)

            path_A.append(A)
            path_k.append(k)
            optimizer_losses.append(loss)

            return loss

        try:
          recommendation = optimizer.minimize(wrapped_objective)
          _, recommended_dict = recommendation.value
          best_A, best_k, best_p = recommended_dict['A'], recommended_dict['k'], recommended_dict['p']

          optimizer_type = type(optimizer).__name__
          print(f"The optimizer selected the {optimizer_type} algorithm.")
          selected_optimizer_cls = optimizer._select_optimizer_cls()
          optimizer_name = selected_optimizer_cls.__class__.__name__ if hasattr(selected_optimizer_cls, "__class__") else "Unknown"
          print(f"The optimizer selected the {optimizer_name} strategy.")

          # Calculate additional metrics
          nrmse_val, adj_r_squared, ks_score = objective_with_metrics((best_A, best_k, best_p), species_data, age_col, dbh_col)
          huber_val = huber_loss_for_params((best_A, best_k, best_p), species_data, age_col, dbh_col)  # Calculate Huber loss

          # Add Huber loss to the new row data
          new_row = pd.DataFrame({
              'species': [species],
              'A': [best_A],
              'k': [best_k],
              'p': [best_p],
              'NRMSE': [nrmse_val],
              'Adj_R2': [adj_r_squared],
              'KS_Score': [ks_score],
              'Huber_Loss': [huber_val]  # Add Huber loss column
              })
          param_results = pd.concat([param_results, new_row], ignore_index=True)


        except Exception as e:
            print(f"Error encountered during optimization for species {species}. Details: {e}")
            continue

        print(f"Nevergrad optimized values for {species} - A: {best_A}, k: {best_k}, p: {best_p}")

        # Create 3D Surface Plot
        num_points = 100
        min_value_A, max_value_A = ranges['A']
        min_value_k, max_value_k = ranges['k']
        fixed_p = recommended_dict['p']

        # Create meshgrid of parameter values
        A_values = np.linspace(min_value_A, max_value_A, num_points)
        k_values = np.linspace(min_value_k, max_value_k, num_points)
        A_grid, k_grid = np.meshgrid(A_values, k_values)

        # Calculate the loss for each combination of parameters
        losses = np.array([objective([A, k, fixed_p], species_data, age_col, dbh_col) for A, k in zip(np.ravel(A_grid), np.ravel(k_grid))])
        loss_grid = losses.reshape(A_grid.shape)

        fig = plt.figure(figsize=(10, 6))
        ax = fig.add_subplot(111, projection="3d")

        # Adjust subplot layout to prevent label cutoff
        plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.95)  # Adjust these values as needed
        surf = ax.plot_surface(A_grid, k_grid, loss_grid, cmap="viridis", alpha=0.6)
        ax.plot(path_A, path_k, optimizer_losses, color="red", marker="o")
        ax.set_xlabel("A")
        ax.set_ylabel("k")
        ax.set_zlabel("Loss")
        ax.zaxis.labelpad = 15  # Adjust the label padding
        plt.title(f"3D Surface Plot of Optimization Landscape for {species}")
        plt.show()

    # Saving results to Excel
    if not param_results.empty:
        param_results.to_excel(os.path.join(output_dir, "numerical/parameters.xlsx"), sheet_name='data', index=False)
    else:
        print("No parameter results to save.")

    return param_results, fitted_values

# Finally, place the main script execution under this condition
if __name__ == '__main__':
    # Start with checking for local files
    species_ranges_df = None
    for file_type in ["csv", "xlsx"]:
        species_ranges_path = f"species_ranges.{file_type}"
        if os.path.exists(species_ranges_path):
            if file_type == "csv":
                species_ranges_df = pd.read_csv(species_ranges_path)
            else:  # For xlsx
                species_ranges_df = pd.read_excel(species_ranges_path)
            print(f"Successfully read species_ranges from {species_ranges_path}.")
            break

    # If no local file was found, try fetching from GitHub
    if species_ranges_df is None:
        species_ranges_url = "https://raw.githubusercontent.com/fixedpointexperimental/Artemis/main/data/species_ranges.csv"
        species_ranges_df = read_species_ranges_from_github(species_ranges_url)

    # If both methods failed, fall back to universal ranges
    if species_ranges_df is None:
        print("Falling back to universal parameter ranges.")

    if DATA is not None:
        cleaned_data = clean_forestry_data(DATA)
        results, values = estimate_chapman_richards(cleaned_data, species_ranges_df=species_ranges_df)

# Here's the flow:
# Check if a local file species_ranges.csv or species_ranges.xlsx is available.
# If not, try fetching from the GitHub URL.
# If that also fails, default to universal ranges.

"""# Plot all fitted Chapman-Richards functions

## Plot all
"""

import seaborn as sns
sns.set_style("whitegrid")  # Set the plotting style

# Define the Chapman-Richards growth function
def chapman_richards(age, A, k, p):
    return A * (1 - np.exp(-k * age))**p

# Read fitted parameter values from Excel file
params_df = pd.read_excel('numerical/parameters.xlsx')

# Create a new figure with a larger size
plt.figure(figsize=(18, 10))

# Define a range of ages for plotting
ages = np.linspace(0, 30, 100)  # Ages from 0 to 30 in 100 steps

# Loop through each species in the parameters file, calculate the growth function, and plot
for index, row in params_df.iterrows():
    species = row['species']
    A = row['A']
    k = row['k']
    p = row['p']
    fitted_values = chapman_richards(ages, A, k, p)
    plt.plot(ages, fitted_values, label=species)  # Plot the fitted values for the species

# Add labels to the axes
plt.xlabel('Age')
plt.ylabel('Fitted dbh')
plt.title('Fitted Values using Chapman-Richards growth function for Different Species')

# Position the legend outside the plot to the right
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

# Save the entire figure as a PNG file
plt.savefig('images/fitted_CR_all_species.png')

# Display the plot
plt.show()

"""## Panel widget"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ipywidgets import interact, fixed  # Import the interact and fixed functions from ipywidgets

# Set the plotting style
sns.set_style("whitegrid")

# Define the Chapman-Richards growth function
def chapman_richards(age, A, k, p):
    return A * (1 - np.exp(-k * age))**p

# Read fitted parameter values from Excel file
params_df = pd.read_excel('numerical/parameters.xlsx')

# Create an interactive panel using ipywidgets
@interact  # Decorate the function with @interact to create an interactive widget
def plot_fitted_values(species=params_df['species'], age_range=(0, 60, 1)):
    # Find the row corresponding to the selected species
    selected_species = params_df[params_df['species'] == species]
    if selected_species.empty:
        print("Species not found in the parameters file.")
        return

    # Get parameters for the selected species
    A = selected_species['A'].values[0]
    k = selected_species['k'].values[0]
    p = selected_species['p'].values[0]

    # Define a range of ages for plotting
    ages = np.linspace(0, age_range, 100)

    # Calculate the fitted values using the growth function
    fitted_values = chapman_richards(ages, A, k, p)

    # Create the plot
    plt.figure(figsize=(10, 6))
    plt.plot(ages, fitted_values, label=species)
    plt.xlabel('Age')
    plt.ylabel('Fitted dbh')
    plt.title(f'Fitted Values using Chapman-Richards growth function for {species}')
    plt.legend()
    plt.show()

"""# Fitted dbh"""

import pandas as pd
import numpy as np

# Load the parameters.xlsx file
parameters_df = pd.read_excel('numerical/parameters.xlsx')

# Load your data
DATA = pd.read_excel('data.xlsx', sheet_name='data')

# Convert 'botanical_names' to lowercase to match 'species' column in parameters_df
DATA['botanical_names'] = DATA['botanical_names'].str.lower()

# Merge data with parameters
merged_data = DATA.merge(parameters_df, left_on='botanical_names', right_on='species', how='inner')

# Define the range of ages
ages = np.arange(1, 31)

# Create a dictionary to store the fitted dbhs
fitted_dbhs_dict = {'age': [], 'species': [], 'fitted_dbh': []}

# Calculate and store fitted dbhs for each age and species
for species in merged_data['botanical_names'].unique():
    species_data = merged_data[merged_data['botanical_names'] == species]
    A = species_data['A'].values[0]
    k = species_data['k'].values[0]
    p = species_data['p'].values[0]

    fitted_dbhs = A * (1 - np.exp(-k * ages))**p

    for i, age in enumerate(ages):
        fitted_dbhs_dict['age'].append(age)
        fitted_dbhs_dict['species'].append(species)
        fitted_dbhs_dict['fitted_dbh'].append(fitted_dbhs[i])

# Create a DataFrame from the dictionary
fitted_dbhs_df = pd.DataFrame(fitted_dbhs_dict)

# Save the DataFrame to an Excel file
fitted_dbhs_df.to_excel('numerical/fitted_dbh.xlsx', sheet_name='data', index=False)

"""# Output files"""

import datetime

# Generate a timestamped filename
current_datetime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
zip_filename = f"artemis_{current_datetime}.zip"

# Zip the folders
!zip -r /content/{zip_filename} /content/numerical /content/images

# Download the zip file
from google.colab import files
files.download(f"/content/{zip_filename}")


